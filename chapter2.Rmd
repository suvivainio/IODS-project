# Regression and model validation

## Description of the dataset

My dataset for this exercise has 7 variables, gender is factor, age, attitude and points are integers and variables deep, stra and surf are double.
```{R echo=F}
learning2014=read.table("./data/learning2014.txt")
print(str(learning2014))
```
First rows of the dataset look like this:
```{R echo=F}
print(head(learning2014))
```
Minimum and maximum values and some other descrptive numbers of the distribution:
```{R echo=F}
print(summary(learning2014))
```

## Variable selection
Below is a visual summary of the relationships in the data. Dependent variable for the linear model is going to be points.

Based on correlation and scatterplots variable Attitude seems a good candidate for an explanatory variable (correlation: 0.4).

As the two other variables for this regression I choose gender and age: it seems that the sign of regression terms might be different for men and women. This I want to investigate further.
```{R echo=F}
library(GGally)
library(ggplot2)

# create a more advanced plot matrix with ggpairs()
p <- ggpairs(learning2014, mapping = aes(col=gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p

```

## Analysis
The only strong correlation that the visual plot matrix above was between points and attitude. It is no surprise that this relationship is statistically significant in the linear model as well (beta:Attitude=0.36066, p < 0.001). Variables gender and age proved to be non-significant in the model.

I also tried separate models for the two genders (not shown here) but age remained insignificant.

```{R echo=F}
model0=lm(learning2014$points ~ learning2014$attitude + 
            learning2014$age + learning2014$gender)
print(summary(model0))

```
Here is the linear model with the only statistically significant parameter attitude. Attitude is positively related to points variable. The final equation is:

points = 11.64 + attitude * 0.35

R-square statistic for this model is 0.1856, in other words attitude alone explains almost 20 % of the variation in points. This is quite a high pecentage! Below there's a visualization of the model
```{R echo=F}
model1=lm(learning2014$points ~ learning2014$attitude)
print(summary(model1))

```
```{R echo=F}
# initialize plot with data and aesthetic mapping
p1 <- ggplot(learning2014, aes(x = attitude, y = points)) +
        geom_point() + 
        stat_smooth(method = "lm") +
        ggtitle("Student's attitude and exam points")
print(p1)

```

## Diagnostic of the model

In the normal model the basic equation is:

y = a + bx + error

Errors are supposed to be independent of y (here: attitude) and follow normal distribution. 

Residuals vs. fitted values plot (below) shows that there exists no correlation or other kind of clear dependence between y values and error values.

Q-Q-plot shows that the standardized residuals approximately follow normal distribution. This means that the assumption of normality can be considered fulfilled.

The residuals versus leverage plot is used to inspect the effect of the observations to the model. Scatter plot of the data and fitted regression line (plot above) has no notable outliers and the leverage plot tells the same story: there are no especially high leverage values.

```{R echo=F}
par(mfrow = c(1,3))
plot(model1, which=c(1,2,5))

```

All in all, after examining the diagnostics' plots it can be concluded that the model is valid.



